{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled24.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "example = ['I go to school.', 'I LIKE pizza!', 'I LIKE apple!', 'I love orange!']"
      ],
      "metadata": {
        "id": "tmdmtO5aqQan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def preprocessing(self, sequences):\n",
        "    import re\n",
        "    result = [re.sub('[^a-zA-Z0-9]',' ', a_sequence).lower().split() for a_sequence in sequences]\n",
        "    return result\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "    index = 1\n",
        "    tokens = self.preprocessing(sequences)\n",
        "    \n",
        "    for token in tokens:\n",
        "      for vocab in token:\n",
        "        if vocab not in self.word_dict:\n",
        "          self.word_dict[vocab] = index\n",
        "          index += 1\n",
        "\n",
        "    self.fit_checker = True\n",
        "  \n",
        "  def transform(self, sequences):\n",
        "    result = []\n",
        "    tokens = self.preprocessing(sequences)\n",
        "    if self.fit_checker:\n",
        "      result = []\n",
        "      for token in tokens:\n",
        "        result.append([self.word_dict[vocab] if vocab in self.word_dict else self.word_dict['oov'] for vocab in token])\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ],
      "metadata": {
        "id": "FVwuC35IB9Mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toke = Tokenizer()"
      ],
      "metadata": {
        "id": "chvmB8lxbCsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = toke.fit_transform(example)\n",
        "tokenized"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vizFVbO8gPB",
        "outputId": "2a2b9f08-e9b6-4f1c-a604-5830308c449d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 2, 3, 4], [1, 5, 6]]"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf_result=[]\n",
        "for token in tokenized:\n",
        "  termfreq_diz = dict.fromkeys(set(sum(tokenized, [])),0)\n",
        "  counter = dict(collections.Counter(token))\n",
        "  for word in token:\n",
        "    termfreq_diz[word]=counter[word]/len(token)\n",
        "  tf_result.append(list(termfreq_diz.values()))\n",
        "result = np.reshape(tf_result, (len(tokenized),-1))"
      ],
      "metadata": {
        "id": "RncDsmvU_SC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf_diz.values()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HfH-MyRCFjmV",
        "outputId": "a25c4c12-f601-467d-e944-2fc4dc59753f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_values([1.0, 1.4054651081081644, 1.4054651081081644, 1.4054651081081644, 1.4054651081081644, 1.4054651081081644])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abc = result * list(idf_diz.values())\n",
        "abc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjvzSw7OETll",
        "outputId": "4773fff2-aa33-42b6-a5e8-766fe47d0beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.25      , 0.35136628, 0.35136628, 0.35136628, 0.        ,\n",
              "        0.        ],\n",
              "       [0.33333333, 0.        , 0.        , 0.        , 0.46848837,\n",
              "        0.46848837]])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final = abc.tolist()\n",
        "final"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-O1RxW8GPKr",
        "outputId": "74d96d08-4d36-4415-b8db-72ea9c45be2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.25, 0.3513662770270411, 0.3513662770270411, 0.3513662770270411, 0.0, 0.0],\n",
              " [0.3333333333333333, 0.0, 0.0, 0.0, 0.4684883693693881, 0.4684883693693881]]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "toke=Tokenizer()"
      ],
      "metadata": {
        "id": "RpooErg_ITsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KYp63AMK25z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer()\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    import collections\n",
        "    import numpy as np\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "    d_sequences = {'bow_{}'.format(i):list(set(b)) for i,b in enumerate(tokenized)}\n",
        "    n=len(d_sequences.keys())\n",
        "    l_sequences = []\n",
        "    for sequence in d_sequences.values():\n",
        "      l_sequences += sequence\n",
        "    counter = dict(collections.Counter(l_sequences))\n",
        "    idf_tokens = dict.fromkeys(set(l_sequences), 0)\n",
        "    for word in set(l_sequences):\n",
        "      idf_tokens[word] = np.log(n/(1+counter[word]))\n",
        "    #return idf_tokens\n",
        "    self.fit_checker = True\n",
        "    \n",
        "\n",
        "  def transform(self, sequences):\n",
        "    import collections\n",
        "    import numpy as np\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "      tf_result=[]\n",
        "      for token in tokenized:\n",
        "        termfreq_diz = dict.fromkeys(set(sum(tokenized, [])),0)\n",
        "        counter = dict(collections.Counter(token))\n",
        "        for word in token:\n",
        "          termfreq_diz[word]=counter[word]/len(token)\n",
        "        tf_result.append(list(termfreq_diz.values()))\n",
        "      result = np.reshape(tf_result, (len(tokenized),-1))\n",
        "      return result\n",
        "      #return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "LI_MNiV6FoHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf = TfidfVectorizer(Tokenizer)"
      ],
      "metadata": {
        "id": "zG1WghTpS70H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf.fit(example)"
      ],
      "metadata": {
        "id": "hfVvh4HuVv-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf.transform(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuSqYPsakUbT",
        "outputId": "7811717f-f81c-4a09-e877-5766b7aef816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.25      , 0.25      , 0.25      , 0.25      , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.33333333, 0.        , 0.        , 0.        , 0.33333333,\n",
              "        0.33333333, 0.        , 0.        , 0.        ],\n",
              "       [0.33333333, 0.        , 0.        , 0.        , 0.33333333,\n",
              "        0.        , 0.33333333, 0.        , 0.        ],\n",
              "       [0.33333333, 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.33333333, 0.33333333]])"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    }
  ]
}