{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugGR7pnI4WSe"
      },
      "source": [
        "# Week3_1 Assignment\n",
        "\n",
        "## [BASIC](#Basic) \n",
        "- 토크나이징이 완료된 위키 백과 코퍼스를 다운받고 **단어 사전을 구축하는 함수를 구현**할 수 있다.\n",
        "- `Skip-Gram` 방식의 학습 데이터 셋을 생성하는 **Dataset과 Dataloader 클래스를 구현**할 수 있다.\n",
        "- **Negative Sampling** 함수를 구현할 수 있다. \n",
        "\n",
        "\n",
        "## [CHALLENGE](#Challenge)\n",
        "- Skip-Gram을 학습 과정 튜토리얼을 따라하며, **Skip-Gram을 학습하는 클래스를 구현**할 수 있다. \n",
        "\n",
        "\n",
        "## [ADVANCED](#Advanced)\n",
        "- Skip-Gram 방식으로 word embedding을 학습하는 **Word2Vec 클래스를 구현**하고 실제로 학습할 수 있다.\n",
        "- 학습이 완료된 word embedding을 불러와 **Gensim 패키지를 사용해 유사한 단어**를 뽑을 수 있다. \n",
        "\n",
        "### Reference\n",
        "- [Skip-Gram negative sampling 한국어 튜토리얼](https://wikidocs.net/69141)\n",
        "    - (참고) 위 튜토리얼에서는 target word와 context word 페어의 레이블은 1로, target word와 negative sample word 페어의 레이블은 0이 되도록 학습 데이터를 구현해 binary classification을 구현한다. 하지만 우리는 word2vec 논문 방식을 그대로 따르기 위해 label을 생성하지 않고 대신 loss 함수를 변행해서 binary classification을 학습할 것이다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:29:36.641276Z",
          "start_time": "2022-02-19T14:29:36.638642Z"
        },
        "id": "HlEy3xfY4WSh"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:50:41.644583Z",
          "start_time": "2022-02-19T12:50:41.642937Z"
        },
        "id": "cBrr7-gt4jnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce3dcc33-2733-4884-f3a7-28d04b385962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 8.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 65.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,>=0.11.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 47.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 65.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.6 transformers-4.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:26:59.276355Z",
          "start_time": "2022-02-19T14:26:58.411434Z"
        },
        "id": "6mC9lhsJ4WSh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:30:05.586472Z",
          "start_time": "2022-02-19T14:30:05.583611Z"
        },
        "id": "17g7UZ5g4WSi"
      },
      "outputs": [],
      "source": [
        "# seed\n",
        "seed = 7777\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:30:06.721039Z",
          "start_time": "2022-02-19T14:30:06.717559Z"
        },
        "id": "v3UlC7Jn4WSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d06bac77-49e0-4a3d-dedd-09c6dd06068e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla T4\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# device type\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8sfv5KY4WSk"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHs8_LU04WSj"
      },
      "source": [
        "### 토크나이징이 완료된 위키 백과 코퍼스 다운로드 및 불용어 사전 크롤링\n",
        "- 나의 구글 드라이브에 데이터를 다운받아 영구적으로 사용할 수 있도록 하자. \n",
        "    - [데이터 다운로드 출처](https://ratsgo.github.io/embedding/downloaddata.html)\n",
        "- 다운받은 데이터는 토크나이징이 완료된 상태이지만 불용어를 포함하고 있다. 따라서 향후 불용어를 제거하기 위해 불용어 사전을 크롤링하자. \n",
        "    - [불용어 사전 출처](https://www.ranks.nl/stopwords/korean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KYiz1fdNsAqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5a8baea-f7fd-49a3-e1bd-887c1cc5e265"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Z2WZ0P4wsAqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6f52741-31f6-441c-abfa-47f9254cbe21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Classroom/AI심화과정\n"
          ]
        }
      ],
      "source": [
        "cd \"/content/drive/MyDrive/Classroom/AI심화과정\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:11.886643Z",
          "start_time": "2022-02-19T14:27:11.884858Z"
        },
        "id": "4QPBJ6UZ4WSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82e6a1f3-3dd9-46e8-9873-6a11e5d4cb7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.2.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.63.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2021.10.8)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "# 데이터 다운로드\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/u/0/uc?id=1Ybp_DmzNEpsBrUKZ1-NoPDzCMO39f-fx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fctmXTUJ62Ms",
        "outputId": "bef443ec-e048-4011-ef6e-79d6b43e0286"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/u/0/uc?id=1Ybp_DmzNEpsBrUKZ1-NoPDzCMO39f-fx \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip tokenized.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrVAWDvT65n8",
        "outputId": "225d8cee-1dad-415d-a5dd-57a72f7e9117"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  tokenized.zip\n",
            "   creating: tokenized/\n",
            "  inflating: tokenized/korquad_mecab.txt  \n",
            "  inflating: tokenized/wiki_ko_mecab.txt  \n",
            "  inflating: tokenized/corpus_mecab_jamo.txt  \n",
            "  inflating: tokenized/ratings_okt.txt  \n",
            "  inflating: tokenized/ratings_khaiii.txt  \n",
            "  inflating: tokenized/ratings_hannanum.txt  \n",
            "  inflating: tokenized/ratings_soynlp.txt  \n",
            "  inflating: tokenized/ratings_mecab.txt  \n",
            "  inflating: tokenized/ratings_komoran.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:15.633947Z",
          "start_time": "2022-02-19T14:27:13.829982Z"
        },
        "id": "cTHCHmO24WSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e2257fb-fe4e-476b-b8f3-3915675d719b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Korean stop words: 677\n"
          ]
        }
      ],
      "source": [
        "# 한국어 불용어 리스트 크롤링\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.ranks.nl/stopwords/korean\"\n",
        "response = requests.get(url, verify = False)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text,'html.parser')\n",
        "    content = soup.select_one('#article178ebefbfb1b165454ec9f168f545239 > div.panel-body > table > tbody > tr')\n",
        "    stop_words=[]\n",
        "    for x in content.strings:\n",
        "        x=x.strip()\n",
        "        if x:\n",
        "            stop_words.append(x)\n",
        "    print(f\"# Korean stop words: {len(stop_words)}\")\n",
        "else:\n",
        "    print(response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:15.642775Z",
          "start_time": "2022-02-19T14:27:15.635333Z"
        },
        "id": "3d0IqhDF4WSk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "302cfc36-2472-4567-afbe-102bfa1588e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'아'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "stop_words[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t76Q1pQ4WSk"
      },
      "source": [
        "### 단어 사전 구축 함수 구현 \n",
        "- 문서 리스트를 입력 받아 사전을 생성하는 `make_vocab()` 함수를 구현하라.\n",
        "- 함수 정의\n",
        "    - 입력 매개변수\n",
        "        - docs : 문서 리스트\n",
        "        - min_count : 최소 단어 등장 빈도수 (단어 빈도가 `min_count` 미만인 단어는 사전에 포함하지 않음)\n",
        "    - 조건\n",
        "        - 문서 길이 제한\n",
        "            - 단어 개수가 3개 이하인 문서는 처리하지 않음. (skip)\n",
        "        - 사전에 포함되는 단어 빈도수 제한\n",
        "            - 단어가 빈도가 `min_count` 미만은 단어는 사전에 포함하지 않음.\n",
        "        - 불용어 제거 \n",
        "            - 불용어 리스트에 포함된 단어는 제거 \n",
        "    - 반환값 \n",
        "        - word2count : 단어별 빈도 사전 (key: 단어, value: 등장 횟수)\n",
        "        - wid2word : 단어별 인덱스(wid) 사전 (key: 단어 인덱스(int), value: 단어)\n",
        "        - word2wid : 인덱스(wid)별 단어 사전 (key: 단어, value: 단어 인덱스(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:01.637431Z",
          "start_time": "2022-02-19T14:32:56.730711Z"
        },
        "id": "xkjqztIA4WSl"
      },
      "outputs": [],
      "source": [
        "# 코퍼스 로드\n",
        "f = open(\"/content/drive/MyDrive/Classroom/AI심화과정/tokenized/ratings_komoran.txt\", 'r')\n",
        "docs_ver0 = f.readlines()\n",
        "docs = [doc.strip() for doc in docs_ver0]\n",
        "f.close()\n",
        "#docs = pd.read_table(\"/content/drive/MyDrive/Classroom/AI심화과정/tokenized/ratings_komoran.txt\", header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:03.423002Z",
          "start_time": "2022-02-19T14:33:03.419818Z"
        },
        "id": "WAKB6bbt4WSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b665bd9e-8470-4926-b2e8-8ae710ba5915"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# wiki documents: 199,992\n"
          ]
        }
      ],
      "source": [
        "print(f\"# wiki documents: {len(docs):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:04.016885Z",
          "start_time": "2022-02-19T14:33:03.962269Z"
        },
        "id": "-OI1MCXv4WSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3fd0f29-e2d1-4fc4-c843-15546e90071a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n"
          ]
        }
      ],
      "source": [
        "# 문서 개수를 500개로 줄임\n",
        "docs=random.sample(docs,500)\n",
        "print(len(docs))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"# wiki documents: {len(docs):,}\")"
      ],
      "metadata": {
        "id": "mP5wGu9YwDUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e39c2bc-be8e-4b0d-a448-623d172fe649"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# wiki documents: 1,000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[:10]"
      ],
      "metadata": {
        "id": "qG2Dy5pdJyWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:26.392627Z",
          "start_time": "2022-02-19T14:33:26.382358Z"
        },
        "id": "aJaEAVm9sAqv"
      },
      "outputs": [],
      "source": [
        "# 문서 내 숫자, 영어 대소문자, 특수문자를 제거 (re package 사용)\n",
        "\n",
        "docs = [re.sub('[^ ㄱ-ㅣ가-힣+]','', a_string) for a_string in docs]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Check : {docs[0][:1000]}\")"
      ],
      "metadata": {
        "id": "sytiSICawMk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf653da-d83f-4f30-ca33-b120d1ba2f4c"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check : 액션 성 도 없 고  마지막 급 마무리 도 그렇 고  주인공 은 총 맞 아도 죽지 도 않 네\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:27.904880Z",
          "start_time": "2022-02-19T14:33:27.899620Z"
        },
        "id": "OAkkQsvO4WSl"
      },
      "outputs": [],
      "source": [
        "def make_vocab(docs:List[str], min_count:int):\n",
        "    \"\"\"\n",
        "    'docs'문서 리스트를 입력 받아 단어 사전을 생성.\n",
        "    \n",
        "    return \n",
        "        - word2count : 단어별 빈도 사전\n",
        "        - wid2word : 단어별 인덱스(wid) 사전 \n",
        "        - word2wid : 인덱스(wid)별 단어 사전\n",
        "    \"\"\"\n",
        "\n",
        "    word2count = dict()\n",
        "    word2id = dict()\n",
        "    id2word = dict()\n",
        "\n",
        "    _word2count = defaultdict(int)\n",
        "\n",
        "    for doc in tqdm(docs):\n",
        "      word_list = doc.split()\n",
        "      \n",
        "        # 1. 문서 길이 제한\n",
        "      if len(word_list) > 3:\n",
        "        # 2. 임시 딕셔너리(_word2count)에 단어별 등장 빈도 기록\n",
        "        for word in word_list:\n",
        "          if word not in stop_words:\n",
        "            _word2count[word] += 1\n",
        "        # 3. 불용어 제거\n",
        "\n",
        "    # 4. 토큰 최소 빈도를 만족하는 토큰만 사전에 추가\n",
        "    for word in _word2count.keys():\n",
        "      if _word2count[word] >= min_count:\n",
        "        word2count[word] = _word2count[word]\n",
        "        if word not in word2id:\n",
        "          word2id[word] = len(id2word)\n",
        "          id2word[word2id[word]] = word\n",
        "    \n",
        "    return word2count, word2id, id2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:30.157872Z",
          "start_time": "2022-02-19T14:33:28.473330Z"
        },
        "id": "ieS5SiQx4WSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6387bd8-9127-416b-91b2-2a998326083d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [00:00<00:00, 6012.34it/s]\n"
          ]
        }
      ],
      "source": [
        "word2count, word2id, id2word = make_vocab(docs, min_count=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word2count.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-tnfdZfnrN5",
        "outputId": "726c3a53-c63f-4d26-c9ef-181d11206434"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "208"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(word2count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZkc9WP6oPeU",
        "outputId": "fe605894-649f-4860-8261-6a13126ff6f0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "203"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(id2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSQrR4eTccFy",
        "outputId": "509ba0ef-18be-4451-8205-039ba6d96472"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "203"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:30.754722Z",
          "start_time": "2022-02-19T14:33:30.752115Z"
        },
        "id": "cT1MRN1EJtx6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9983d8a-bcd4-46a9-cd06-05f54b86b5b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4,274\n"
          ]
        }
      ],
      "source": [
        "doc_len = sum(word2count.values()) # 문서 내 모든 단어의 개수 (단어별 등장 빈도의 총 합)\n",
        "print(f\"{doc_len:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:32.916830Z",
          "start_time": "2022-02-19T14:33:32.914355Z"
        },
        "id": "e_1MneB54WSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0cecf33-699a-4df2-850b-cc6580cb4854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# unique word : 203\n"
          ]
        }
      ],
      "source": [
        "print(f\"# unique word : {len(word2id):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHxtZqtk4WSm"
      },
      "source": [
        "### Dataset 클래스 구현\n",
        "- Skip-Gram 방식의 학습 데이터 셋(`Tuple(target_word, context_word)`)을 생성하는 `CustomDataset` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()` 함수) 입력 매개변수\n",
        "        - docs: 문서 리스트\n",
        "        - word2id: 단어별 인덱스(wid) 사전\n",
        "        - window_size: Skip-Gram의 윈도우 사이즈\n",
        "    - 메소드\n",
        "        - `make_pair()`\n",
        "            - 문서를 단어로 쪼개고, 사전에 존재하는 단어들만 단어 인덱스로 변경\n",
        "            - Skip-gram 방식의 `(target_word, context_word)` 페어(tuple)들을 `pairs` 리스트에 담아 반환\n",
        "        - `__len__()`\n",
        "            - `pairs` 리스트의 개수 반환\n",
        "        - `__getitem__(index)`\n",
        "            - `pairs` 리스트를 인덱싱\n",
        "    - 주의 사항\n",
        "        - `nn.Module`를 부모 클래스로 상속 받음 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.111290Z",
          "start_time": "2022-02-19T14:33:38.104531Z"
        },
        "id": "UPiLcYCZ4WSm"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    문서 리스트를 받아 skip-gram 방식의 (target_word, context_word) 데이터 셋을 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, docs:List[str], word2id:Dict[str,int], window_size:int=5):\n",
        "        self.docs = docs\n",
        "        self.word2id = word2id\n",
        "        self.window_size = window_size\n",
        "        self.pairs = self.make_pair()\n",
        "    \n",
        "    def make_pair(self):\n",
        "        \"\"\"\n",
        "        (target, context) 형식의 Skip-gram pair 데이터 셋 생성 \n",
        "        \"\"\"\n",
        "\n",
        "        pairs = []\n",
        "\n",
        "        for doc in self.docs:\n",
        "          word_id_list = [self.word2id[word] for word in doc.split() if word in self.word2id]\n",
        "          if len(word_id_list) >= 2:\n",
        "            for idx, id in enumerate(word_id_list):\n",
        "              for num in range(self.window_size // 2):\n",
        "                if idx - (num+1) > 0:\n",
        "                  pairs.append((id, word_id_list[idx - (num+1)]))\n",
        "                if idx + (num+1) <= (len(word_id_list)-1):\n",
        "                  pairs.append((id, word_id_list[idx + (num+1)]))\n",
        "\n",
        "        return pairs\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.945361Z",
          "start_time": "2022-02-19T14:33:38.385577Z"
        },
        "id": "YntOw2q94WSm"
      },
      "outputs": [],
      "source": [
        "dataset = CustomDataset(docs, word2id, window_size=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.949614Z",
          "start_time": "2022-02-19T14:33:38.946663Z"
        },
        "id": "-RpNbAjk4WSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16bb2292-bd4b-470e-aa01-7a4a924f4041"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13545"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:43.072635Z",
          "start_time": "2022-02-19T14:33:43.069526Z"
        },
        "id": "1FBwcL4H4WSn"
      },
      "outputs": [],
      "source": [
        "dataset.pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:51.040595Z",
          "start_time": "2022-02-19T14:33:51.031473Z"
        },
        "id": "wTAwTjKk4WSn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# verify (target word, context word)\n",
        "for i, pair in enumerate(dataset):\n",
        "    if i==100:\n",
        "        break\n",
        "    print(f\"({id2word[pair[0]]}, {id2word[pair[1]]})\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Z50-Dr4WSn"
      },
      "source": [
        "### 위에서 생성한 `dataset`으로 DataLoader  객체 생성\n",
        "- `DataLoader` 클래스로 `train_dataloader`객체를 생성하라. \n",
        "    - 생성자 매개변수와 값\n",
        "        - dataset = 위에서 생성한 dataset\n",
        "        - batch_size = 64\n",
        "        - shuffle = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:02.645176Z",
          "start_time": "2022-02-19T14:34:02.642780Z"
        },
        "id": "GXcAvFB14WSn"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(dataset, batch_size = 64, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:02.777322Z",
          "start_time": "2022-02-19T14:34:02.774335Z"
        },
        "id": "4Yfcwi_14WSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b0a63d-9db0-4dee-d3ba-007c4586586c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "212"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTs16gsU4WSn"
      },
      "source": [
        "### Negative Sampling 함수 구현\n",
        "- Skip-Gram은 복잡도를 줄이기 위한 방법으로 negative sampling을 사용한다. \n",
        "- `sample_table`이 다음과 같이 주어졌을 때, sample_table에서 랜덤으로 값을 뽑아 (batch_size, n_neg_sample) shape의 matrix를 반환하는 `get_neg_v_negative_sampling()`함수를 구현하라. \n",
        "- Sample Table은 negative distribution을 따른다. \n",
        "    - [negative distribution 설명](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#How-are-negative-samples-drawn?)\n",
        "- 함수 정의\n",
        "    - 입력 매개변수\n",
        "        - batch_size : 배치 사이즈, matrix의 row 개수 \n",
        "        - n_neg_sample : negative sample의 개수, matrix의 column 개수\n",
        "    - 반환값 \n",
        "        - neg_v : 추출된 negative sample (2차원의 리스트)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.397509Z",
          "start_time": "2022-02-19T14:34:11.386389Z"
        },
        "id": "PUqIB6dH4WSn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# negative sample을 추출할 sample table 생성 (해당 코드를 참고)\n",
        "sample_table = []\n",
        "sample_table_size = doc_len"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# noise distribution 생성\n",
        "alpha = 3/4\n",
        "frequency_list = np.array(list(word2count.values())) ** alpha\n",
        "Z = sum(frequency_list)\n",
        "ratio = frequency_list/Z\n",
        "negative_sample_dist = np.round(ratio*sample_table_size)"
      ],
      "metadata": {
        "id": "bu2TZexT5enb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_sample_dist"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PyzBfyjL5kzo",
        "outputId": "ff71b5cf-ccab-48c7-a5b8-ea93ad89207b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 11.,  15., 105.,  72.,  78.,  26.,  37., 211.,  11., 130.,  25.,\n",
              "        22.,  26., 100.,  72., 115.,  60.,  15.,  43., 147.,   8.,  15.,\n",
              "        11.,  24.,  70.,  36., 113.,  63.,   8.,  30.,  54.,  30.,  13.,\n",
              "        20.,  21.,  15.,  44.,   9.,  14.,  17.,  12.,  19.,  24.,  23.,\n",
              "        38.,  12.,  22.,  13.,  12.,  64.,  31.,  32.,  15.,  13.,  34.,\n",
              "        19.,  76.,  43.,  11.,  31.,   8.,   8.,   9.,  13.,  11.,   8.,\n",
              "        15.,  22.,  28.,  35.,  29.,  36.,  15.,  36.,   9.,  41.,   8.,\n",
              "        15.,  18.,  17.,  46.,  28.,  28.,  28.,   8.,  18.,  15.,  20.,\n",
              "        58.,  12.,  55.,  14.,  23.,  13.,   8.,  13.,  30.,  36.,  12.,\n",
              "        13.,   8.,   8.,  11.,   8.,  11.,  30.,  24.,   9.,  27.,  15.,\n",
              "        33.,  18.,   8.,  27.,  12.,   8.,   9.,  22.,  14.,   9.,  11.,\n",
              "         9.,  13.,  14.,  18.,  22.,   8.,   8.,  11.,   8.,   9.,  11.,\n",
              "        14.,   8.,  14.,  12.,  11.,   8.,  21.,   9.,  11.,   9.,   9.,\n",
              "        13.,  16.,  13.,  16.,  14.,  11.,  25.,   8.,  13.,   9.,   8.,\n",
              "        11.,   9.,  17.,  13.,  18.,  14.,   8.,  23.,  13.,  20.,  20.,\n",
              "        11.,  11.,   8.,  38.,  12.,  14.,   8.,  13.,  15.,   9.,  13.,\n",
              "        17.,  16.,  18.,   8.,  11.,   8.,  13.,  11.,   9.,  12.,  12.,\n",
              "         9.,  13.,  16.,   8.,   8.,   9.,   9.,   8.,   8.,  11.,   9.,\n",
              "        14.,  11.,  14.,   9.,   9.,  14.,   8.,  26.,   8.,   8.,   8.,\n",
              "         8.,   8.,   9.,  13.,   8.,  11.,   8.,  12.,   9.,   8.,   8.,\n",
              "         8.,   9.,   9.,   9.,   9.,   8.,  22.,   9.,   8.])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for wid, c in enumerate(negative_sample_dist):\n",
        "    sample_table.extend([wid]*int(c))"
      ],
      "metadata": {
        "id": "9WOl-ENH5hDw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_table[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xlc8uOqD6sdw",
        "outputId": "86804c68-4ec7-4b51-c8aa-3f5e45491608"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.508414Z",
          "start_time": "2022-02-19T14:34:11.505464Z"
        },
        "id": "Wdu8qK8x4WSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3414bac9-9a94-4d70-8dcc-c3178bf763f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4947"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "len(sample_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.656046Z",
          "start_time": "2022-02-19T14:34:11.653325Z"
        },
        "id": "mQIVrOIR4WSn"
      },
      "outputs": [],
      "source": [
        "def get_neg_v_negative_sampling(batch_size:int, n_neg_sample:int):\n",
        "    \"\"\"\n",
        "    위에서 정의한 sample_table에서 (batch_size, n_neg_sample) shape만큼 랜덤 추출해 \"네거티브 샘플 메트릭스\"를 생성\n",
        "    np.random.choice() 함수 활용 (위에서 정의한 sample_table을 함수의 argument로 사용)\n",
        "    \"\"\"\n",
        "    neg_v = []\n",
        "    for _ in range(batch_size):\n",
        "      neg_v.append(np.random.choice(sample_table, n_neg_sample))\n",
        "    \n",
        "    return neg_v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:12.345976Z",
          "start_time": "2022-02-19T14:34:12.333448Z"
        },
        "id": "8wwT4Af04WSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a30f540-78a7-40e9-ca79-23b4d7a7e174"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([140,  61,  53,   8,  18]),\n",
              " array([193,   8,  78,   8, 123]),\n",
              " array([  2,   1,  12, 186,  78]),\n",
              " array([15, 23, 39, 76, 67])]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "get_neg_v_negative_sampling(4, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLnDXPvJ4WSo"
      },
      "source": [
        "## Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5UubCzK4WSo"
      },
      "source": [
        "### 미니 튜토리얼\n",
        "- 아래 튜토리얼을 따라하며 Skip-Gram 모델의 `forward` 및 `loss` 연산 방식을 이해하자\n",
        "- Reference\n",
        "    - [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "    - [torch bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n",
        "    - [Skip-Gram negative sampling loss function 설명 영문 블로그](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#Derivation-of-Cost-Function-in-Negative-Sampling)\n",
        "    - [Skip-Gram negative sampling loss function 설명 한글 블로그](https://reniew.github.io/22/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:46.954048Z",
          "start_time": "2022-02-19T12:51:46.951529Z"
        },
        "id": "IAR68hsY4WSo"
      },
      "outputs": [],
      "source": [
        "# hyper parameter example\n",
        "emb_size = 30000 # vocab size\n",
        "emb_dimension = 300 # word embedding 차원\n",
        "n_neg_sample = 5\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.340056Z",
          "start_time": "2022-02-19T12:51:47.300999Z"
        },
        "id": "zzOsVUn94WSo"
      },
      "outputs": [],
      "source": [
        "# 1. Embedding Matrix와 Context Matrix를 생성\n",
        "u_embedding = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device)\n",
        "v_embedding = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.352240Z",
          "start_time": "2022-02-19T12:51:49.341437Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7J_ADc44WSo",
        "outputId": "f5affb6b-4c73-498b-d4ff-81f818e9f5a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target word idx : tensor([24460, 10634,  2864, 23952,  3320, 15187, 19625, 26546, 27339,  3920,\n",
            "        25847,  6023,  5055,  7070,  6291, 10245, 15926,   641, 20178,  4565,\n",
            "         4784, 26715, 16955, 28742, 17947, 19774,  8065, 22605,  3061, 28965,\n",
            "         3056, 17963]) Pos context word idx : tensor([23224,  5636, 23712,  5234,  3991, 17897, 25123, 17938, 19634, 24228,\n",
            "          693,   799, 25457,  1308, 28935, 25696,  5601, 23878,  8312,  1292,\n",
            "        21380, 16974,  9318,  9578, 12915, 29271, 26465, 20572,  2362, 25929,\n",
            "        19754, 29080]) Neg context word idx : [array([ 7, 47, 13, 90, 88]), array([171, 113,  26,  85, 117]), array([163, 184,  25, 169,  72]), array([ 17,  19,  14, 158, 107]), array([ 45, 222, 161,  19,  31]), array([ 67, 105,  13, 137, 177]), array([  1, 108,  63,  19, 207]), array([ 14, 129,  90, 192,  56]), array([ 31,  27, 119,  66, 150]), array([ 29,   9, 159,  74, 172]), array([ 54,   0,  97,   7, 160]), array([175,  64, 220,  24,  10]), array([ 36,  81, 200, 144, 138]), array([ 27,   7,  19, 153,  61]), array([184, 163, 164,  15,  30]), array([ 7, 71, 46, 18, 56]), array([97, 12, 31,  7, 16]), array([165,  26,  13,  46, 177]), array([  9, 118, 223,  87, 106]), array([164,  19,  41,  14,  54]), array([ 26,  56,  23,  13, 124]), array([ 90, 105, 157,  71,  14]), array([56, 10, 53, 24, 31]), array([ 19, 189,  50,  74, 170]), array([ 80,  46, 206,  27, 180]), array([ 15, 156, 214, 136,  49]), array([ 26, 178,  14, 159,  42]), array([ 7, 13, 27, 14, 41]), array([168,  97, 163,  32,  16]), array([ 56,  12, 187,  80,  19]), array([ 79,  82,  44, 105,   7]), array([ 19,  81, 107, 157,  48])]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2. wid(단어 인덱스)를 임의로 생성\n",
        "pos_u = torch.randint(high = emb_size, size = (batch_size,))\n",
        "pos_v = torch.randint(high = emb_size, size = (batch_size,))\n",
        "neg_v = get_neg_v_negative_sampling(batch_size, n_neg_sample)\n",
        "print(f\"Target word idx : {pos_u} Pos context word idx : {pos_v} Neg context word idx : {neg_v}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.364020Z",
          "start_time": "2022-02-19T12:51:49.353486Z"
        },
        "id": "4iEG0nCZ4WSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a2eb35f-4071-4175-a292-03a8fdc32b16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "# 3. tensor로 변환\n",
        "pos_u = Variable(torch.LongTensor(pos_u)).to(device)\n",
        "pos_v = Variable(torch.LongTensor(pos_v)).to(device)\n",
        "neg_v = Variable(torch.LongTensor(neg_v)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:51.391896Z",
          "start_time": "2022-02-19T12:51:51.387084Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqbNbajG4WSo",
        "outputId": "0e40dde8-1ffb-4f68-96b9-b37419fc20c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of pos_u embedding : torch.Size([32, 300])\n",
            " shape of pos_v embedding : torch.Size([32, 300])\n",
            " shape of neg_v embedding : torch.Size([32, 5, 300])\n"
          ]
        }
      ],
      "source": [
        "# 4. wid로 각각의 embedding matrix에서 word embedding 값을 가져오기\n",
        "pos_u = u_embedding(pos_u)\n",
        "pos_v = v_embedding(pos_v)\n",
        "neg_v = v_embedding(neg_v)\n",
        "print(f\"shape of pos_u embedding : {pos_u.shape}\\n shape of pos_v embedding : {pos_v.shape}\\n shape of neg_v embedding : {neg_v.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:53.121477Z",
          "start_time": "2022-02-19T12:51:52.646148Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDWUrSwo4WSo",
        "outputId": "593f6b45-2aa1-4fda-b9ca-7ea0f76b371f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of pos logits : torch.Size([32])\n",
            "\n",
            "shape of logits : torch.Size([32, 5])\n"
          ]
        }
      ],
      "source": [
        "# 5. dot product \n",
        "pos_score = torch.mul(pos_u, pos_v) # 행렬 element-wise 곱\n",
        "pos_score = torch.sum(pos_score, dim=1)\n",
        "print(f\"shape of pos logits : {pos_score.shape}\\n\")\n",
        "\n",
        "neg_score = torch.bmm(neg_v, pos_u.unsqueeze(dim=2)).squeeze()\n",
        "print(f\"shape of logits : {neg_score.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:53.670418Z",
          "start_time": "2022-02-19T12:51:53.665671Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adOpcoL54WSo",
        "outputId": "b62e17b5-c780-4874-9f9c-edc6c318d0cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos logits : -231.58099365234375\n",
            "neg logits : -1022.7206420898438\n",
            "Loss : 1254.3016357421875\n"
          ]
        }
      ],
      "source": [
        "# 6. loss 구하기\n",
        "pos_score = F.logsigmoid(pos_score)\n",
        "neg_score = F.logsigmoid(-1*neg_score) # negative의 logit은 minimize 하기 위해 -1 곱함\n",
        "print(f\"pos logits : {pos_score.sum()}\")\n",
        "print(f\"neg logits : {neg_score.sum()}\")\n",
        "loss = -1 * (torch.sum(pos_score) + torch.sum(neg_score))\n",
        "print(f\"Loss : {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muEceOGZ4WSo"
      },
      "source": [
        "### Skip-gram 클래스 구현\n",
        "- Skip-Gram 방식으로 단어 embedding을 학습하는 `SkipGram` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()` 함수) 입력 매개변수\n",
        "        - `vocab_size` : 사전내 단어 개수\n",
        "        - `emb_dimension` : 엠베딩 크기\n",
        "        - `device` : 연산 장치 종류\n",
        "    - 생성자에서 생성해야할 변수 \n",
        "        - `vocab_size` : 사전내 단어 개수\n",
        "        - `emb_dimension` : 엠베딩 크기\n",
        "        - `u_embedding` : (vocab_size, emb_dimension) 엠베딩 메트릭스 (target_word)\n",
        "        - `v_embedding` : (vocab_size, emb_dimension) 엠베딩 메트릭스 (context_word)\n",
        "    - 메소드\n",
        "        - `init_embedding()` (제공됨)\n",
        "            - 엠베딩 메트릭스 값을 초기화\n",
        "        - `forward()`\n",
        "            - 위 튜토리얼과 같이 dot product를 수행한 후 score를 생성\n",
        "            - loss를 반환 (loss 설명 추가)\n",
        "        - `save_emedding()` (제공됨)\n",
        "            - `u_embedding`의 단어 엠베딩 값을 단어 별로 파일에 저장\n",
        "    - 주의 사항     \n",
        "        - `nn.Module`를 부모 클래스로 상속 받음 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:15.731306Z",
          "start_time": "2022-02-19T14:34:15.721129Z"
        },
        "id": "pnmMamP44WSo"
      },
      "outputs": [],
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size:int, emb_dimension:int, device:str):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embedding = nn.Embedding(self.vocab_size, self.emb_dimension, sparse=True).to(device)\n",
        "        self.v_embedding = nn.Embedding(self.vocab_size, self.emb_dimension, sparse=True).to(device)\n",
        "        self.init_embedding()\n",
        "    \n",
        "    \n",
        "    def init_embedding(self):\n",
        "        \"\"\"\n",
        "        u_embedding과 v_embedding 메트릭스 값을 초기화\n",
        "        \"\"\"\n",
        "        initrange = 0.5 / self.emb_dimension\n",
        "        self.u_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embedding.weight.data.uniform_(-0, 0)\n",
        "    \n",
        "    \n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        \"\"\"\n",
        "        dot product를 수행한 후 score를 생성\n",
        "        loss 반환\n",
        "        \"\"\"    \n",
        "            \n",
        "        # 각각의 embedding matrix에서 word embedding 값을 가져오기\n",
        "        pos_u = self.u_embedding(pos_u)\n",
        "        pos_v = self.v_embedding(pos_v)\n",
        "        neg_v = self.v_embedding(neg_v)\n",
        "        \n",
        "        # dot product \n",
        "        pos_score = F.logsigmoid(torch.sum(torch.mul(pos_u, pos_v), dim=1))\n",
        "        neg_score = F.logsigmoid(-1*(torch.bmm(neg_v, pos_u.unsqueeze(dim=2)).squeeze()))\n",
        "        \n",
        "        # loss 구하기\n",
        "        loss = -1 * (torch.sum(pos_score) + torch.sum(neg_score))\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def save_embedding(self, id2word, file_name, use_cuda):\n",
        "        \"\"\"\n",
        "        'file_name' 위치에 word와 word_embedding을 line-by로 저장\n",
        "        파일의 첫 줄은 '단어 개수' 그리고 '단어 embedding 사이즈' 값을 입력해야 함\n",
        "        \"\"\"\n",
        "        if use_cuda: # parameter를 gpu 메모리에서 cpu 메모리로 옮김\n",
        "            embedding = self.u_embedding.weight.cpu().data.numpy()\n",
        "        else:\n",
        "            embedding = self.u_embedding.weight.data.numpy()\n",
        "\n",
        "        with open(file_name, 'w') as writer:\n",
        "            # 파일의 첫 줄은 '단어 개수' 그리고 '단어 embedding 사이즈' 값을 입력해야 함\n",
        "            writer.write(f\"{len(id2word)} {embedding.shape[-1]}\\n\")\n",
        "            \n",
        "            for wid, word in id2word.items():\n",
        "                e = embedding[wid]\n",
        "                e = \" \".join([str(e_) for e_ in e])\n",
        "                writer.write(f\"{word} {e}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqqMo0zL4WSo"
      },
      "source": [
        "## Advanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSWd5gV24WSp"
      },
      "source": [
        "### Skip-Gram 방식의  Word2Vec 클래스 구현\n",
        "- Skip-Gram 방식으로 단어 embedding을 학습하는 `Word2Vec` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()`) 입력 매개 변수\n",
        "        - `input_file` : 학습할 문서 리스트\n",
        "        - `output_file_name` : 학습된 word embedding을 저장할 파일 위치\n",
        "        - `device` : 연상 장치 종류\n",
        "        - `emb_dimension` : word embedding 차원\n",
        "        - `batch_size` : 학습 배치 사이즈\n",
        "        - `window_size` : skip-gram 윈도우 사이즈 (context word 개수를 결정)\n",
        "        - `n_neg_sample` : negative sample 개수\n",
        "        - `iteration` : 학습 반복 횟수\n",
        "        - `lr` : learning rate\n",
        "        - `min_count` : 사전에 추가될 단어의 최소 등장 빈도\n",
        "    - 생성자에서 생성해야 할 변수 \n",
        "        - `docs` : 학습할 문서 리스트\n",
        "        - `output_file_name` : 학습된 word embedding을 저장할 파일 위치\n",
        "        - `word2count`, `word2id`, `id2word` : 위에서 구현한 `make_vocab()` 함수의 반환 값\n",
        "        - `device` : 연산 장치 종류\n",
        "        - `emb_size` : vocab의 (unique한) 단어 종류 \n",
        "        - `emb_dimension` : word embedding 차원\n",
        "        - `batch_size` : 학습 배치 사이즈\n",
        "        - `window_size` : skip-gram 윈도우 사이즈 (context word 개수를 결정)\n",
        "        - `n_neg_sample` : negative sample 개수\n",
        "        - `iteration` : 학습 반복 횟수\n",
        "        - `lr` : learning rate\n",
        "        - `model` : `SkipGram` 클래스의 인스턴스\n",
        "        - `optimizer` : `SGD` 클래스의 인스턴스\n",
        "    - 메소드\n",
        "        - `train()`\n",
        "            - 입력 매개변수 \n",
        "                - `train_dataloader`\n",
        "            - Iteration 횟수만큼 input_file 학습 데이터를 학습한다. 매 epoch마다 for loop 돌면서 batch 단위 학습 데이터를 skip gram 모델에 학습함. 학습이 끝나면 word embedding을 output_file_name 파일에 저장.\n",
        "- Reference\n",
        "    - [Optimizer - SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:20.503555Z",
          "start_time": "2022-02-19T14:34:20.491585Z"
        },
        "id": "Td-GQrqI4WSp"
      },
      "outputs": [],
      "source": [
        "class Word2Vec:\n",
        "    def __init__(self, \n",
        "                input_file: List[str],\n",
        "                output_file_name: str,\n",
        "                 device: str,\n",
        "                 emb_dimension=300,\n",
        "                 batch_size = 64,\n",
        "                 window_size=5,\n",
        "                 n_neg_sample = 5,\n",
        "                 iteration=1,\n",
        "                 lr = 0.02,\n",
        "                 min_count=5):\n",
        "        self.docs = input_file\n",
        "        self.output_file_name = output_file_name\n",
        "        self.word2count, self.word2id, self.id2word = make_vocab(self.docs, min_count)\n",
        "        self.device = device\n",
        "        self.emb_size = len(self.word2count)\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.batch_size = batch_size\n",
        "        self.window_size = window_size\n",
        "        self.n_neg_sample = n_neg_sample\n",
        "        self.iteration = iteration\n",
        "        self.lr = lr\n",
        "        self.model = SkipGram(vocab_size = self.emb_size, emb_dimension = self.emb_dimension, device = self.device)\n",
        "        self.optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr) # torch.optim.SGD 클래스 사용\n",
        "\n",
        "        # train() 함수에서 만든 임베딩 결과 파일들을 저장할 폴더 생성 (os.makedirs 사용)\n",
        "        os.makedirs(self.output_file_name, exist_ok=True)\n",
        "        \n",
        "    \n",
        "    def train(self, train_dataloader):\n",
        "        \n",
        "        # lr 값을 조절하는 스케줄러 인스턴스 변수를 생성\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer = self.optimizer,\n",
        "            num_warmup_steps=0,\n",
        "            num_training_steps=(len(train_dataloader)*self.iteration)\n",
        "        )\n",
        "        \n",
        "        for epoch in range(self.iteration):\n",
        "            \n",
        "            print(f\"*****Epoch {epoch} Train Start*****\")\n",
        "            print(f\"*****Epoch {epoch} Total Step {len(train_dataloader)}*****\")\n",
        "            total_loss, batch_loss, batch_step = 0,0,0\n",
        "\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                batch_step+=1\n",
        "\n",
        "                pos_u, pos_v = batch\n",
        "                # negative data 생성\n",
        "                neg_v = get_neg_v_negative_sampling(pos_u.shape[0], self.n_neg_sample)\n",
        "                \n",
        "                # 데이터를 tensor화 & device 설정\n",
        "                pos_u = Variable(torch.LongTensor(pos_u)).to(device)\n",
        "                pos_v = Variable(torch.LongTensor(pos_v)).to(device)\n",
        "                neg_v = Variable(torch.LongTensor(neg_v)).to(device)\n",
        "\n",
        "                # model의 gradient 초기화\n",
        "                self.model.zero_grad() \n",
        "                # optimizer의 gradient 초기화\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                loss = self.model.forward(pos_u, pos_v, neg_v)\n",
        "\n",
        "                # loss 계산\n",
        "                loss.backward()\n",
        "                # optimizer 업데이트\n",
        "                self.optimizer.step() \n",
        "                # scheduler 업데이트\n",
        "                self.scheduler.step()\n",
        "\n",
        "                batch_loss += loss.item()\n",
        "                total_loss += loss.item()\n",
        "                \n",
        "                if (step%500 == 0) and (step!=0):\n",
        "                    print(f\"Step: {step} Loss: {batch_loss/batch_step:.4f} lr: {self.optimizer.param_groups[0]['lr']:.4f}\")\n",
        "                    # 변수 초기화    \n",
        "                    batch_loss, batch_step = 0,0\n",
        "            \n",
        "            print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "            print(f\"*****Epoch {epoch} Train Finished*****\\n\")\n",
        "            \n",
        "            print(f\"*****Epoch {epoch} Saving Embedding...*****\")\n",
        "            self.model.save_embedding(self.id2word, os.path.join(self.output_file_name, f'w2v_{epoch}.txt'), True if 'cuda' in self.device.type else False)\n",
        "            print(f\"*****Epoch {epoch} Embedding Saved at {os.path.join(self.output_file_name, f'w2v_{epoch}.txt')}*****\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:29.561892Z",
          "start_time": "2022-02-19T14:34:26.103659Z"
        },
        "id": "Ywx9R8n24WSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e836c8d1-6178-4892-aa55-b083550cdaec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 199992/199992 [00:31<00:00, 6286.07it/s]\n"
          ]
        }
      ],
      "source": [
        "output_file = os.path.join(\".\", \"word2vec_wiki\")\n",
        "# Word2Vec 클래스의 인스턴스 생성\n",
        "w2v = Word2Vec(docs, output_file, device, n_neg_sample=10, iteration=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.batch_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7Z1_Pa48Fs3",
        "outputId": "f210c8ea-320b-4639-b5a9-dab3ee99673a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:34.615469Z",
          "start_time": "2022-02-19T14:34:34.055502Z"
        },
        "id": "ufBxjKxN4WSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8c76b86-58ec-44f8-cbbc-0f082c0f275e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "137613"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ],
      "source": [
        "# 학습 데이터 셋 및 데이터 로더 생성 (위에서 생성한 w2v의 attribute들을 argument에 적절히 넣기)\n",
        "dataset = CustomDataset(w2v.docs, w2v.word2id, window_size=5)\n",
        "train_dataloader = DataLoader(dataset, batch_size = w2v.batch_size, shuffle = True)\n",
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:45:38.362817Z",
          "start_time": "2022-02-19T14:34:37.382371Z"
        },
        "id": "9JBUrUJ34WSp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "be9c1051-332f-4b25-8ce5-d607969df6a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Epoch 0 Train Start*****\n",
            "*****Epoch 0 Total Step 137613*****\n",
            "Step: 500 Loss: 307.2390 lr: 0.0199\n",
            "Step: 1000 Loss: 164.9097 lr: 0.0199\n",
            "Step: 1500 Loss: 142.8845 lr: 0.0198\n",
            "Step: 2000 Loss: 132.8422 lr: 0.0197\n",
            "Step: 2500 Loss: 126.5764 lr: 0.0196\n",
            "Step: 3000 Loss: 122.5197 lr: 0.0196\n",
            "Step: 3500 Loss: 119.3651 lr: 0.0195\n",
            "Step: 4000 Loss: 115.5862 lr: 0.0194\n",
            "Step: 4500 Loss: 113.6893 lr: 0.0193\n",
            "Step: 5000 Loss: 111.0491 lr: 0.0193\n",
            "Step: 5500 Loss: 110.7597 lr: 0.0192\n",
            "Step: 6000 Loss: 108.4746 lr: 0.0191\n",
            "Step: 6500 Loss: 107.9082 lr: 0.0191\n",
            "Step: 7000 Loss: 106.4603 lr: 0.0190\n",
            "Step: 7500 Loss: 105.3848 lr: 0.0189\n",
            "Step: 8000 Loss: 104.5083 lr: 0.0188\n",
            "Step: 8500 Loss: 104.8265 lr: 0.0188\n",
            "Step: 9000 Loss: 102.6500 lr: 0.0187\n",
            "Step: 9500 Loss: 102.2071 lr: 0.0186\n",
            "Step: 10000 Loss: 102.1126 lr: 0.0185\n",
            "Step: 10500 Loss: 101.9213 lr: 0.0185\n",
            "Step: 11000 Loss: 101.8166 lr: 0.0184\n",
            "Step: 11500 Loss: 99.8215 lr: 0.0183\n",
            "Step: 12000 Loss: 100.0155 lr: 0.0183\n",
            "Step: 12500 Loss: 99.4841 lr: 0.0182\n",
            "Step: 13000 Loss: 99.9118 lr: 0.0181\n",
            "Step: 13500 Loss: 98.8725 lr: 0.0180\n",
            "Step: 14000 Loss: 99.0041 lr: 0.0180\n",
            "Step: 14500 Loss: 98.8077 lr: 0.0179\n",
            "Step: 15000 Loss: 98.1540 lr: 0.0178\n",
            "Step: 15500 Loss: 98.7414 lr: 0.0177\n",
            "Step: 16000 Loss: 97.3528 lr: 0.0177\n",
            "Step: 16500 Loss: 98.3285 lr: 0.0176\n",
            "Step: 17000 Loss: 97.4344 lr: 0.0175\n",
            "Step: 17500 Loss: 97.5369 lr: 0.0175\n",
            "Step: 18000 Loss: 96.8554 lr: 0.0174\n",
            "Step: 18500 Loss: 96.1485 lr: 0.0173\n",
            "Step: 19000 Loss: 97.0858 lr: 0.0172\n",
            "Step: 19500 Loss: 96.7457 lr: 0.0172\n",
            "Step: 20000 Loss: 95.6824 lr: 0.0171\n",
            "Step: 20500 Loss: 96.0994 lr: 0.0170\n",
            "Step: 21000 Loss: 95.6845 lr: 0.0169\n",
            "Step: 21500 Loss: 94.8300 lr: 0.0169\n",
            "Step: 22000 Loss: 95.6312 lr: 0.0168\n",
            "Step: 22500 Loss: 95.4247 lr: 0.0167\n",
            "Step: 23000 Loss: 96.0860 lr: 0.0167\n",
            "Step: 23500 Loss: 94.9421 lr: 0.0166\n",
            "Step: 24000 Loss: 94.9728 lr: 0.0165\n",
            "Step: 24500 Loss: 95.4168 lr: 0.0164\n",
            "Step: 25000 Loss: 94.7637 lr: 0.0164\n",
            "Step: 25500 Loss: 94.3679 lr: 0.0163\n",
            "Step: 26000 Loss: 94.5306 lr: 0.0162\n",
            "Step: 26500 Loss: 94.5727 lr: 0.0161\n",
            "Step: 27000 Loss: 94.6822 lr: 0.0161\n",
            "Step: 27500 Loss: 94.5546 lr: 0.0160\n",
            "Step: 28000 Loss: 94.1563 lr: 0.0159\n",
            "Step: 28500 Loss: 93.4168 lr: 0.0159\n",
            "Step: 29000 Loss: 94.8516 lr: 0.0158\n",
            "Step: 29500 Loss: 94.0252 lr: 0.0157\n",
            "Step: 30000 Loss: 93.7127 lr: 0.0156\n",
            "Step: 30500 Loss: 92.9037 lr: 0.0156\n",
            "Step: 31000 Loss: 93.4356 lr: 0.0155\n",
            "Step: 31500 Loss: 92.8804 lr: 0.0154\n",
            "Step: 32000 Loss: 93.6680 lr: 0.0153\n",
            "Step: 32500 Loss: 92.4276 lr: 0.0153\n",
            "Step: 33000 Loss: 93.6071 lr: 0.0152\n",
            "Step: 33500 Loss: 93.7206 lr: 0.0151\n",
            "Step: 34000 Loss: 92.8818 lr: 0.0151\n",
            "Step: 34500 Loss: 92.7594 lr: 0.0150\n",
            "Step: 35000 Loss: 93.6880 lr: 0.0149\n",
            "Step: 35500 Loss: 93.0745 lr: 0.0148\n",
            "Step: 36000 Loss: 92.9945 lr: 0.0148\n",
            "Step: 36500 Loss: 91.6466 lr: 0.0147\n",
            "Step: 37000 Loss: 92.7560 lr: 0.0146\n",
            "Step: 37500 Loss: 92.2669 lr: 0.0145\n",
            "Step: 38000 Loss: 92.3149 lr: 0.0145\n",
            "Step: 38500 Loss: 92.0537 lr: 0.0144\n",
            "Step: 39000 Loss: 93.0009 lr: 0.0143\n",
            "Step: 39500 Loss: 92.6717 lr: 0.0143\n",
            "Step: 40000 Loss: 92.9530 lr: 0.0142\n",
            "Step: 40500 Loss: 92.0756 lr: 0.0141\n",
            "Step: 41000 Loss: 91.9092 lr: 0.0140\n",
            "Step: 41500 Loss: 92.3269 lr: 0.0140\n",
            "Step: 42000 Loss: 92.2147 lr: 0.0139\n",
            "Step: 42500 Loss: 91.2911 lr: 0.0138\n",
            "Step: 43000 Loss: 91.7907 lr: 0.0138\n",
            "Step: 43500 Loss: 91.3419 lr: 0.0137\n",
            "Step: 44000 Loss: 92.5364 lr: 0.0136\n",
            "Step: 44500 Loss: 91.5598 lr: 0.0135\n",
            "Step: 45000 Loss: 92.5190 lr: 0.0135\n",
            "Step: 45500 Loss: 92.0518 lr: 0.0134\n",
            "Step: 46000 Loss: 91.6611 lr: 0.0133\n",
            "Step: 46500 Loss: 93.0573 lr: 0.0132\n",
            "Step: 47000 Loss: 91.9230 lr: 0.0132\n",
            "Step: 47500 Loss: 92.3103 lr: 0.0131\n",
            "Step: 48000 Loss: 91.5962 lr: 0.0130\n",
            "Step: 48500 Loss: 91.5608 lr: 0.0130\n",
            "Step: 49000 Loss: 91.7297 lr: 0.0129\n",
            "Step: 49500 Loss: 91.3214 lr: 0.0128\n",
            "Step: 50000 Loss: 91.0155 lr: 0.0127\n",
            "Step: 50500 Loss: 91.3386 lr: 0.0127\n",
            "Step: 51000 Loss: 91.8983 lr: 0.0126\n",
            "Step: 51500 Loss: 91.1111 lr: 0.0125\n",
            "Step: 52000 Loss: 90.9728 lr: 0.0124\n",
            "Step: 52500 Loss: 91.2761 lr: 0.0124\n",
            "Step: 53000 Loss: 90.7183 lr: 0.0123\n",
            "Step: 53500 Loss: 90.5778 lr: 0.0122\n",
            "Step: 54000 Loss: 91.4977 lr: 0.0122\n",
            "Step: 54500 Loss: 91.0862 lr: 0.0121\n",
            "Step: 55000 Loss: 91.7791 lr: 0.0120\n",
            "Step: 55500 Loss: 91.2737 lr: 0.0119\n",
            "Step: 56000 Loss: 90.2706 lr: 0.0119\n",
            "Step: 56500 Loss: 91.2196 lr: 0.0118\n",
            "Step: 57000 Loss: 91.0842 lr: 0.0117\n",
            "Step: 57500 Loss: 91.2860 lr: 0.0116\n",
            "Step: 58000 Loss: 89.9335 lr: 0.0116\n",
            "Step: 58500 Loss: 91.3826 lr: 0.0115\n",
            "Step: 59000 Loss: 90.5837 lr: 0.0114\n",
            "Step: 59500 Loss: 91.1039 lr: 0.0114\n",
            "Step: 60000 Loss: 91.1364 lr: 0.0113\n",
            "Step: 60500 Loss: 89.8227 lr: 0.0112\n",
            "Step: 61000 Loss: 90.7796 lr: 0.0111\n",
            "Step: 61500 Loss: 90.2391 lr: 0.0111\n",
            "Step: 62000 Loss: 90.1466 lr: 0.0110\n",
            "Step: 62500 Loss: 90.7871 lr: 0.0109\n",
            "Step: 63000 Loss: 90.5737 lr: 0.0108\n",
            "Step: 63500 Loss: 89.9180 lr: 0.0108\n",
            "Step: 64000 Loss: 90.3034 lr: 0.0107\n",
            "Step: 64500 Loss: 90.8015 lr: 0.0106\n",
            "Step: 65000 Loss: 90.5786 lr: 0.0106\n",
            "Step: 65500 Loss: 89.4904 lr: 0.0105\n",
            "Step: 66000 Loss: 91.1182 lr: 0.0104\n",
            "Step: 66500 Loss: 90.4777 lr: 0.0103\n",
            "Step: 67000 Loss: 90.2073 lr: 0.0103\n",
            "Step: 67500 Loss: 90.5323 lr: 0.0102\n",
            "Step: 68000 Loss: 90.3311 lr: 0.0101\n",
            "Step: 68500 Loss: 88.9765 lr: 0.0100\n",
            "Step: 69000 Loss: 89.6548 lr: 0.0100\n",
            "Step: 69500 Loss: 90.1473 lr: 0.0099\n",
            "Step: 70000 Loss: 90.4283 lr: 0.0098\n",
            "Step: 70500 Loss: 90.4363 lr: 0.0098\n",
            "Step: 71000 Loss: 89.3973 lr: 0.0097\n",
            "Step: 71500 Loss: 89.8182 lr: 0.0096\n",
            "Step: 72000 Loss: 89.6725 lr: 0.0095\n",
            "Step: 72500 Loss: 89.4636 lr: 0.0095\n",
            "Step: 73000 Loss: 89.5685 lr: 0.0094\n",
            "Step: 73500 Loss: 89.0072 lr: 0.0093\n",
            "Step: 74000 Loss: 89.6896 lr: 0.0092\n",
            "Step: 74500 Loss: 90.1833 lr: 0.0092\n",
            "Step: 75000 Loss: 89.9420 lr: 0.0091\n",
            "Step: 75500 Loss: 89.6554 lr: 0.0090\n",
            "Step: 76000 Loss: 89.8926 lr: 0.0090\n",
            "Step: 76500 Loss: 89.1900 lr: 0.0089\n",
            "Step: 77000 Loss: 89.5592 lr: 0.0088\n",
            "Step: 77500 Loss: 89.8887 lr: 0.0087\n",
            "Step: 78000 Loss: 88.6989 lr: 0.0087\n",
            "Step: 78500 Loss: 89.3735 lr: 0.0086\n",
            "Step: 79000 Loss: 89.0716 lr: 0.0085\n",
            "Step: 79500 Loss: 88.8692 lr: 0.0084\n",
            "Step: 80000 Loss: 89.8307 lr: 0.0084\n",
            "Step: 80500 Loss: 89.2176 lr: 0.0083\n",
            "Step: 81000 Loss: 89.9027 lr: 0.0082\n",
            "Step: 81500 Loss: 89.1426 lr: 0.0082\n",
            "Step: 82000 Loss: 89.9930 lr: 0.0081\n",
            "Step: 82500 Loss: 89.6639 lr: 0.0080\n",
            "Step: 83000 Loss: 88.2951 lr: 0.0079\n",
            "Step: 83500 Loss: 89.3344 lr: 0.0079\n",
            "Step: 84000 Loss: 89.4832 lr: 0.0078\n",
            "Step: 84500 Loss: 89.7921 lr: 0.0077\n",
            "Step: 85000 Loss: 89.5788 lr: 0.0076\n",
            "Step: 85500 Loss: 88.6628 lr: 0.0076\n",
            "Step: 86000 Loss: 89.1021 lr: 0.0075\n",
            "Step: 86500 Loss: 88.6884 lr: 0.0074\n",
            "Step: 87000 Loss: 89.2505 lr: 0.0074\n",
            "Step: 87500 Loss: 89.0800 lr: 0.0073\n",
            "Step: 88000 Loss: 89.0388 lr: 0.0072\n",
            "Step: 88500 Loss: 89.9436 lr: 0.0071\n",
            "Step: 89000 Loss: 88.9726 lr: 0.0071\n",
            "Step: 89500 Loss: 88.7836 lr: 0.0070\n",
            "Step: 90000 Loss: 89.5153 lr: 0.0069\n",
            "Step: 90500 Loss: 89.4747 lr: 0.0068\n",
            "Step: 91000 Loss: 89.9069 lr: 0.0068\n",
            "Step: 91500 Loss: 88.4921 lr: 0.0067\n",
            "Step: 92000 Loss: 89.0946 lr: 0.0066\n",
            "Step: 92500 Loss: 89.1949 lr: 0.0066\n",
            "Step: 93000 Loss: 89.4462 lr: 0.0065\n",
            "Step: 93500 Loss: 89.1796 lr: 0.0064\n",
            "Step: 94000 Loss: 88.9114 lr: 0.0063\n",
            "Step: 94500 Loss: 88.8723 lr: 0.0063\n",
            "Step: 95000 Loss: 89.0523 lr: 0.0062\n",
            "Step: 95500 Loss: 89.6318 lr: 0.0061\n",
            "Step: 96000 Loss: 88.8512 lr: 0.0060\n",
            "Step: 96500 Loss: 87.9202 lr: 0.0060\n",
            "Step: 97000 Loss: 89.0846 lr: 0.0059\n",
            "Step: 97500 Loss: 89.1401 lr: 0.0058\n",
            "Step: 98000 Loss: 88.8708 lr: 0.0058\n",
            "Step: 98500 Loss: 88.6568 lr: 0.0057\n",
            "Step: 99000 Loss: 88.4912 lr: 0.0056\n",
            "Step: 99500 Loss: 89.7173 lr: 0.0055\n",
            "Step: 100000 Loss: 89.2663 lr: 0.0055\n",
            "Step: 100500 Loss: 88.8433 lr: 0.0054\n",
            "Step: 101000 Loss: 89.0519 lr: 0.0053\n",
            "Step: 101500 Loss: 88.8377 lr: 0.0052\n",
            "Step: 102000 Loss: 89.0644 lr: 0.0052\n",
            "Step: 102500 Loss: 88.4745 lr: 0.0051\n",
            "Step: 103000 Loss: 89.2416 lr: 0.0050\n",
            "Step: 103500 Loss: 89.6530 lr: 0.0050\n",
            "Step: 104000 Loss: 89.2978 lr: 0.0049\n",
            "Step: 104500 Loss: 88.3033 lr: 0.0048\n",
            "Step: 105000 Loss: 89.0003 lr: 0.0047\n",
            "Step: 105500 Loss: 89.0085 lr: 0.0047\n",
            "Step: 106000 Loss: 88.4274 lr: 0.0046\n",
            "Step: 106500 Loss: 88.3801 lr: 0.0045\n",
            "Step: 107000 Loss: 87.9342 lr: 0.0044\n",
            "Step: 107500 Loss: 87.9883 lr: 0.0044\n",
            "Step: 108000 Loss: 88.7526 lr: 0.0043\n",
            "Step: 108500 Loss: 88.3448 lr: 0.0042\n",
            "Step: 109000 Loss: 87.9744 lr: 0.0042\n",
            "Step: 109500 Loss: 87.8487 lr: 0.0041\n",
            "Step: 110000 Loss: 87.6189 lr: 0.0040\n",
            "Step: 110500 Loss: 89.0991 lr: 0.0039\n",
            "Step: 111000 Loss: 89.1954 lr: 0.0039\n",
            "Step: 111500 Loss: 88.1540 lr: 0.0038\n",
            "Step: 112000 Loss: 88.6054 lr: 0.0037\n",
            "Step: 112500 Loss: 88.5398 lr: 0.0036\n",
            "Step: 113000 Loss: 87.4002 lr: 0.0036\n",
            "Step: 113500 Loss: 88.4461 lr: 0.0035\n",
            "Step: 114000 Loss: 88.4181 lr: 0.0034\n",
            "Step: 114500 Loss: 87.4905 lr: 0.0034\n",
            "Step: 115000 Loss: 88.1665 lr: 0.0033\n",
            "Step: 115500 Loss: 89.5468 lr: 0.0032\n",
            "Step: 116000 Loss: 88.3037 lr: 0.0031\n",
            "Step: 116500 Loss: 88.5371 lr: 0.0031\n",
            "Step: 117000 Loss: 88.2156 lr: 0.0030\n",
            "Step: 117500 Loss: 87.8781 lr: 0.0029\n",
            "Step: 118000 Loss: 88.5458 lr: 0.0029\n",
            "Step: 118500 Loss: 88.4311 lr: 0.0028\n",
            "Step: 119000 Loss: 88.0234 lr: 0.0027\n",
            "Step: 119500 Loss: 88.1818 lr: 0.0026\n",
            "Step: 120000 Loss: 88.0030 lr: 0.0026\n",
            "Step: 120500 Loss: 87.4269 lr: 0.0025\n",
            "Step: 121000 Loss: 88.0940 lr: 0.0024\n",
            "Step: 121500 Loss: 88.2629 lr: 0.0023\n",
            "Step: 122000 Loss: 87.9110 lr: 0.0023\n",
            "Step: 122500 Loss: 88.6160 lr: 0.0022\n",
            "Step: 123000 Loss: 88.2407 lr: 0.0021\n",
            "Step: 123500 Loss: 88.4042 lr: 0.0021\n",
            "Step: 124000 Loss: 87.8148 lr: 0.0020\n",
            "Step: 124500 Loss: 87.7086 lr: 0.0019\n",
            "Step: 125000 Loss: 87.5422 lr: 0.0018\n",
            "Step: 125500 Loss: 88.0229 lr: 0.0018\n",
            "Step: 126000 Loss: 88.1299 lr: 0.0017\n",
            "Step: 126500 Loss: 88.1337 lr: 0.0016\n",
            "Step: 127000 Loss: 88.6102 lr: 0.0015\n",
            "Step: 127500 Loss: 88.5244 lr: 0.0015\n",
            "Step: 128000 Loss: 88.1413 lr: 0.0014\n",
            "Step: 128500 Loss: 88.2594 lr: 0.0013\n",
            "Step: 129000 Loss: 87.7303 lr: 0.0013\n",
            "Step: 129500 Loss: 88.7048 lr: 0.0012\n",
            "Step: 130000 Loss: 88.0241 lr: 0.0011\n",
            "Step: 130500 Loss: 88.5085 lr: 0.0010\n",
            "Step: 131000 Loss: 87.8629 lr: 0.0010\n",
            "Step: 131500 Loss: 88.0108 lr: 0.0009\n",
            "Step: 132000 Loss: 88.6378 lr: 0.0008\n",
            "Step: 132500 Loss: 87.8388 lr: 0.0007\n",
            "Step: 133000 Loss: 88.4228 lr: 0.0007\n",
            "Step: 133500 Loss: 88.0217 lr: 0.0006\n",
            "Step: 134000 Loss: 88.8179 lr: 0.0005\n",
            "Step: 134500 Loss: 87.7139 lr: 0.0005\n",
            "Step: 135000 Loss: 87.7997 lr: 0.0004\n",
            "Step: 135500 Loss: 88.1208 lr: 0.0003\n",
            "Step: 136000 Loss: 87.4234 lr: 0.0002\n",
            "Step: 136500 Loss: 88.7380 lr: 0.0002\n",
            "Step: 137000 Loss: 87.5779 lr: 0.0001\n",
            "Step: 137500 Loss: 88.3383 lr: 0.0000\n",
            "Epoch 0 Total Mean Loss : 93.3858\n",
            "*****Epoch 0 Train Finished*****\n",
            "\n",
            "*****Epoch 0 Saving Embedding...*****\n",
            "*****Epoch 0 Embedding Saved at ./word2vec_wiki/w2v_0.txt*****\n",
            "\n",
            "*****Epoch 1 Train Start*****\n",
            "*****Epoch 1 Total Step 137613*****\n",
            "Step: 500 Loss: 87.7189 lr: 0.0000\n",
            "Step: 1000 Loss: 86.4063 lr: 0.0000\n",
            "Step: 1500 Loss: 87.4315 lr: 0.0000\n",
            "Step: 2000 Loss: 87.1973 lr: 0.0000\n",
            "Step: 2500 Loss: 87.4890 lr: 0.0000\n",
            "Step: 3000 Loss: 87.7801 lr: 0.0000\n",
            "Step: 3500 Loss: 87.4954 lr: 0.0000\n",
            "Step: 4000 Loss: 88.4710 lr: 0.0000\n",
            "Step: 4500 Loss: 87.5328 lr: 0.0000\n",
            "Step: 5000 Loss: 87.8342 lr: 0.0000\n",
            "Step: 5500 Loss: 86.7543 lr: 0.0000\n",
            "Step: 6000 Loss: 88.4312 lr: 0.0000\n",
            "Step: 6500 Loss: 86.8336 lr: 0.0000\n",
            "Step: 7000 Loss: 87.7344 lr: 0.0000\n",
            "Step: 7500 Loss: 87.1133 lr: 0.0000\n",
            "Step: 8000 Loss: 87.9963 lr: 0.0000\n",
            "Step: 8500 Loss: 87.3988 lr: 0.0000\n",
            "Step: 9000 Loss: 86.7245 lr: 0.0000\n",
            "Step: 9500 Loss: 87.3334 lr: 0.0000\n",
            "Step: 10000 Loss: 86.8011 lr: 0.0000\n",
            "Step: 10500 Loss: 87.6128 lr: 0.0000\n",
            "Step: 11000 Loss: 87.6286 lr: 0.0000\n",
            "Step: 11500 Loss: 87.1008 lr: 0.0000\n",
            "Step: 12000 Loss: 88.8038 lr: 0.0000\n",
            "Step: 12500 Loss: 88.3298 lr: 0.0000\n",
            "Step: 13000 Loss: 87.7656 lr: 0.0000\n",
            "Step: 13500 Loss: 87.1922 lr: 0.0000\n",
            "Step: 14000 Loss: 87.1267 lr: 0.0000\n",
            "Step: 14500 Loss: 87.3215 lr: 0.0000\n",
            "Step: 15000 Loss: 88.0924 lr: 0.0000\n",
            "Step: 15500 Loss: 87.2067 lr: 0.0000\n",
            "Step: 16000 Loss: 87.7146 lr: 0.0000\n",
            "Step: 16500 Loss: 87.3111 lr: 0.0000\n",
            "Step: 17000 Loss: 88.2801 lr: 0.0000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-119-d47ce37a6bb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 학습\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-47-8d969913e491>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_dataloader)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mpos_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;31m# negative data 생성\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mneg_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_neg_v_negative_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_u\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_neg_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;31m# 데이터를 tensor화 & device 설정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-90033d39b2f3>\u001b[0m in \u001b[0;36mget_neg_v_negative_sampling\u001b[0;34m(batch_size, n_neg_sample)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mneg_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0mneg_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neg_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mneg_v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# 학습\n",
        "w2v.train(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uTIm4vJ4WSp"
      },
      "source": [
        "### 유사한 단어 확인\n",
        "- 사전에 존재하는 단어들과 유사한 단어를 검색해보자. Gensim 패키지는 유사 단어 외에도 단어간의 유사도를 계산하는 여러 함수를 제공한다. 실험을 통해 word2vec의 한계점을 발견했다면 아래에 markdown으로 작성해보자. \n",
        "- [Gensim 패키지 document](https://radimrehurek.com/gensim/models/keyedvectors.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:47:59.605389Z",
          "start_time": "2022-02-19T14:47:59.368925Z"
        },
        "id": "AKpBuVlP4WSp"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:49:06.590460Z",
          "start_time": "2022-02-19T14:49:05.174241Z"
        },
        "id": "AWTCodimsAq8"
      },
      "outputs": [],
      "source": [
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Classroom/AI심화과정/word2vec_wiki/w2v_0.txt', binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:49:11.324372Z",
          "start_time": "2022-02-19T14:49:11.315429Z"
        },
        "id": "MLMh_evrsAq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7eee264-2c24-40a9-d702-6d6078f86a6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('여자', 0.9355576038360596),\n",
              " ('신', 0.8800905346870422),\n",
              " ('얼굴', 0.8793779015541077),\n",
              " ('분', 0.8790296316146851),\n",
              " ('코', 0.8759520053863525),\n",
              " ('랑', 0.87357097864151),\n",
              " ('작가', 0.8725649118423462),\n",
              " ('감독', 0.8651474118232727),\n",
              " ('씨', 0.8651243448257446),\n",
              " ('이랑', 0.8625461459159851)]"
            ]
          },
          "metadata": {},
          "execution_count": 124
        }
      ],
      "source": [
        "word_vectors.most_similar(positive='남자')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "word2vec의 한계점은? 단어의 의미를 중심으로 유사도를 비교하는 것이 아니고 단어의 출현 위치를 중심으로 유사도를 비교, 이 한계점은 출현 위치가 비슷하면 유사한 의미를 가진다는 가정에서 발생함."
      ],
      "metadata": {
        "id": "X8lc8NQe4cT2"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "9팀 박명건(Week3-Day1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}