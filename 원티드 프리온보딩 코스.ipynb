{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "최종 수정 중...?",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self):\n",
        "    self.word_dict = {'oov': 0}\n",
        "    self.fit_checker = False\n",
        "\n",
        "  def preprocessing(self, sequences):\n",
        "    result = []\n",
        "\n",
        "    import re\n",
        "    \"\"\"\n",
        "    result = [re.sub('[^a-zA-Z0-9]',' ', a_sequence).lower().split() for a_sequence in sequences]\n",
        "    \"\"\"\n",
        "    #조건 1: 입력된 문장에 대해서 소문자로의 변환과 특수문자 제거를 수행합니다.\n",
        "    #조건 2: 토큰화는 white space 단위로 수행합니다.\n",
        "    #output: 각 문장을 토큰화한 결과로, nested list 형태입니다.\n",
        "    for a_sequence in sequences:\n",
        "      result.append(re.sub('[^a-zA-Z0-9]',' ', a_sequence).lower().split())\n",
        "\n",
        "    return result\n",
        "\n",
        "  def fit(self, sequences):\n",
        "    self.fit_checker = False\n",
        "\n",
        "    index = 1\n",
        "    #조건 1: 위에서 만든 preprocessing 함수를 이용하여 각 문장에 대해 토큰화를 수행합니다.\n",
        "    for token in self.preprocessing(sequences):\n",
        "      for word in token:\n",
        "        if word not in self.word_dict:\n",
        "          #조건 2: 각각의 토큰을 정수 인덱싱 하기 위한 어휘 사전(`self.word_dict`)을 생성합니다.\n",
        "          self.word_dict[word] = index\n",
        "          index += 1\n",
        "          \n",
        "    self.fit_checker = True\n",
        "  \n",
        "  def transform(self, sequences):\n",
        "    result = []\n",
        "    tokens = self.preprocessing(sequences)\n",
        "    if self.fit_checker:\n",
        "\n",
        "      for token in tokens:\n",
        "        #조건 1: 어휘 사전(self.word_dict)에 없는 단어는 'oov'의 index로 변환합니다.\n",
        "        #output: 각 문장의 정수 인덱싱으로, nested list 형태입니다.\n",
        "        result.append([self.word_dict[vocab] if vocab in self.word_dict else self.word_dict['oov'] for vocab in token])\n",
        "\n",
        "      return result\n",
        "    else:\n",
        "      raise Exception(\"Tokenizer instance is not fitted yet.\")\n",
        "      \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    result = self.transform(sequences)\n",
        "    return result"
      ],
      "metadata": {
        "id": "FVwuC35IB9Mu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TfidfVectorizer:\n",
        "  def __init__(self, tokenizer):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.fit_checker = False\n",
        "  \n",
        "  def fit(self, sequences):\n",
        "    tokenized = self.tokenizer.fit_transform(sequences)\n",
        "\n",
        "    self.idf = []\n",
        "    import collections\n",
        "    import numpy as np\n",
        "    d_sequences = {'bow_{}'.format(i):list(set(b)) for i,b in enumerate(tokenized)}\n",
        "    #n : 입력된 전체 문장 개수\n",
        "    n=len(d_sequences.keys())\n",
        "    l_sequences = sum(d_sequences.values(),[])\n",
        "    #df(d,t) : 단어 t가 포함된 문장 d의 개수\n",
        "    df = dict(collections.Counter(l_sequences))\n",
        "    idf_tokens = dict.fromkeys(set(l_sequences), 0)\n",
        "    for word in set(l_sequences):\n",
        "      idf_tokens[word] = np.log(n/(1+df[word]))\n",
        "    #조건 1: IDF 행렬은 list 형태입니다.\n",
        "    self.idf.extend(list(idf_tokens.values()))\n",
        "\n",
        "    self.fit_checker = True\n",
        "    \n",
        "  def transform(self, sequences):\n",
        "    if self.fit_checker:\n",
        "      tokenized = self.tokenizer.transform(sequences)\n",
        "\n",
        "      self.tfidf_matrix = []\n",
        "      import collections\n",
        "      import numpy as np\n",
        "      tf_matrix=[]\n",
        "      for token in tokenized:\n",
        "        tf_dic = dict.fromkeys(set(sum(tokenized, [])),0)\n",
        "        tf_counter = dict(collections.Counter(token))\n",
        "        for word in token:\n",
        "          tf_dic[word]=tf_counter[word]\n",
        "        #조건1 : 입력 문장을 이용해 TF 행렬(tf_matrix)을 만드세요.\n",
        "        #tf(d, t) : 문장 d에 단어 t가 나타난 횟수\n",
        "        tf_matrix.append(list(tf_dic.values()))\n",
        "      result = np.reshape(tf_matrix, (len(tokenized),-1))\n",
        "\n",
        "      #조건2 : 문제 2-1( fit())에서 만든 IDF 행렬과 아래 식을 이용해 TF-IDF 행렬을 만드세요\n",
        "      tf_idf = result * self.idf\n",
        "\n",
        "      #output : nested list 형태입니다.\n",
        "      self.tfidf_matrix.extend(tf_idf.tolist())\n",
        "\n",
        "      return self.tfidf_matrix\n",
        "    else:\n",
        "      raise Exception(\"TfidfVectorizer instance is not fitted yet.\")\n",
        "  \n",
        "  def fit_transform(self, sequences):\n",
        "    self.fit(sequences)\n",
        "    return self.transform(sequences)"
      ],
      "metadata": {
        "id": "LI_MNiV6FoHz"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}